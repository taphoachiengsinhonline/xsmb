const tf = require('@tensorflow/tfjs-node');
const Result = require('../models/Result');
const NNPrediction = require('../models/NNPrediction');
const NNState = require('../models/NNState');
const { DateTime } = require('luxon');
const FeatureEngineeringService = require('./featureEngineeringService');

const NN_MODEL_NAME = 'GDB_LSTM_TFJS';
const SEQUENCE_LENGTH = 7;
const OUTPUT_NODES = 50;
const EPOCHS = 50;
const BATCH_SIZE = 32;
const MODEL_VERSION = 'v1.0'; // Th√™m version control cho model state (tƒÉng khi thay ƒë·ªïi features ho·∫∑c architecture)

class TensorFlowService {
  constructor() {
    this.model = null;
    this.featureService = new FeatureEngineeringService();
    this.inputNodes = 0;
  }

  async buildModel(inputNodes) {
  this.inputNodes = inputNodes;
  this.model = tf.sequential({
    layers: [
      // Th√™m Bidirectional LSTM cho layer ƒë·∫ßu ti√™n
      tf.layers.bidirectional({
        layer: tf.layers.lstm({
          units: 128,
          returnSequences: true,
          inputShape: [SEQUENCE_LENGTH, inputNodes]
        }),
        mergeMode: 'concat' // K·∫øt h·ª£p output t·ª´ forward v√† backward
      }),
      tf.layers.dropout({ rate: 0.2 }),
      
      // Th√™m Multi-Head Attention ƒë·ªÉ focus v√†o c√°c ph·∫ßn quan tr·ªçng c·ªßa sequence
      tf.layers.multiHeadAttention({
        numHeads: 4,       // S·ªë heads
        headSize: 32,      // K√≠ch th∆∞·ªõc m·ªói head
        outputSize: 128,   // Output size
        useBias: true
      }),
      
      tf.layers.lstm({
        units: 64,
        returnSequences: false
      }),
      tf.layers.dropout({ rate: 0.2 }),
      tf.layers.dense({
        units: 32,
        activation: 'relu'
      }),
      tf.layers.dense({
        units: OUTPUT_NODES,
        activation: 'sigmoid'
      })
    ]
  });

  this.model.compile({
    optimizer: tf.train.adam(0.001),
    loss: 'binaryCrossentropy',
    metrics: ['accuracy']
  });

  console.log('‚úÖ TensorFlow LSTM model built with Bidirectional and Attention mechanisms');
  return this.model;
}

  async trainModel(trainingSplit) {
  const { trainData, valData } = trainingSplit;

  const classWeights = this.calculateClassWeights(trainData.map(d => d.targetArray));

  const trainInputs = trainData.map(d => d.inputSequence);
  const trainTargets = trainData.map(d => d.targetArray);
  const valInputs = valData.map(d => d.inputSequence);
  const valTargets = valData.map(d => d.targetArray);

  const trainInputTensor = tf.tensor3d(trainInputs);
  const trainTargetTensor = tf.tensor2d(trainTargets);
  const valInputTensor = tf.tensor3d(valInputs);
  const valTargetTensor = tf.tensor2d(valTargets);

  const k = 5;
  const foldSize = Math.floor(trainInputs.length / k);
  let histories = [];

  for (let fold = 0; fold < k; fold++) {
    const foldStart = fold * foldSize;
    const foldEnd = (fold + 1) * foldSize;

    const foldValInputs = trainInputs.slice(foldStart, foldEnd);
    const foldValTargets = trainTargets.slice(foldStart, foldEnd);
    const foldTrainInputs = trainInputs.slice(0, foldStart).concat(trainInputs.slice(foldEnd));
    const foldTrainTargets = trainTargets.slice(0, foldStart).concat(trainTargets.slice(foldEnd));

    const foldTrainTensorX = tf.tensor3d(foldTrainInputs);
    const foldTrainTensorY = tf.tensor2d(foldTrainTargets);
    const foldValTensorX = tf.tensor3d(foldValInputs);
    const foldValTensorY = tf.tensor2d(foldValTargets);

    const history = await this.model.fit(foldTrainTensorX, foldTrainTensorY, {
      epochs: EPOCHS,
      batchSize: BATCH_SIZE,
      validationData: [foldValTensorX, foldValTensorY],
      classWeight: classWeights,
      callbacks: {
        onEpochEnd: (epoch, logs) => {
          console.log(`Fold ${fold + 1} - Epoch ${epoch + 1}: Loss = ${logs.loss.toFixed(4)}`);
        }
      }
    });

    histories.push(history);

    foldTrainTensorX.dispose();
    foldTrainTensorY.dispose();
    foldValTensorX.dispose();
    foldValTensorY.dispose();
  }

  trainInputTensor.dispose();
  trainTargetTensor.dispose();
  valInputTensor.dispose();
  valTargetTensor.dispose();

  return histories;
}

  async predict(inputSequence) {
    const inputTensor = tf.tensor3d([inputSequence], [1, SEQUENCE_LENGTH, this.inputNodes]);
    const prediction = this.model.predict(inputTensor);
    const output = await prediction.data();
    prediction.dispose();
    inputTensor.dispose();
    return Array.from(output);
  }

  prepareTarget(gdbString) {
    const target = Array(OUTPUT_NODES).fill(0.01);
    gdbString.split('').forEach((digit, index) => {
      const d = parseInt(digit);
      if (!isNaN(d) && index < 5) {
        target[index * 10 + d] = 0.99;
      }
    });
    return target;
  }

  async prepareTrainingData() {
  const results = await Result.find().sort({ 'ngay': 1 }).lean();
  if (results.length < SEQUENCE_LENGTH + 1) {
    throw new Error(`Kh√¥ng ƒë·ªß d·ªØ li·ªáu. C·∫ßn √≠t nh·∫•t ${SEQUENCE_LENGTH + 1} ng√†y.`);
  }

  const grouped = {};
  results.forEach(r => {
    if (!grouped[r.ngay]) grouped[r.ngay] = [];
    grouped[r.ngay].push(r);
  });

  const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
  const trainingData = [];

  for (let i = 0; i < days.length - SEQUENCE_LENGTH; i++) {
    const sequenceDays = days.slice(i, i + SEQUENCE_LENGTH);
    const targetDay = days[i + SEQUENCE_LENGTH];

    const previousDays = [];
    const inputSequence = sequenceDays.map(day => {
      const dayResults = grouped[day] || [];
      const prevDays = previousDays.slice();
      previousDays.push(dayResults);
      return this.featureService.extractAllFeatures(dayResults, prevDays, day);
    });

    const targetGDB = (grouped[targetDay] || []).find(r => r.giai === 'ƒêB');
    if (targetGDB?.so && String(targetGDB.so).length >= 5) {
      const targetGDBString = String(targetGDB.so).padStart(5, '0');
      const targetArray = this.prepareTarget(targetGDBString);
      trainingData.push({ inputSequence, targetArray });
    }
  }

  if (trainingData.length > 0) {
    this.inputNodes = trainingData[0].inputSequence[0].length;
  }

  console.log(`üìä Prepared ${trainingData.length} training sequences v·ªõi feature size: ${this.inputNodes}`);

  const total = trainingData.length;
  const trainEnd = Math.floor(total * 0.8);
  const valEnd = Math.floor(total * 0.9);

  const trainData = trainingData.slice(0, trainEnd);
  const valData = trainingData.slice(trainEnd, valEnd);
  const testData = trainingData.slice(valEnd);

  console.log(`üìä Split data: Train ${trainData.length}, Val ${valData.length}, Test ${testData.length}`);
  return { trainData, valData, testData };
}

  dateKey(s) {
    if (!s || typeof s !== 'string') return '';
    const parts = s.split('/');
    return parts.length !== 3 ? s : `${parts[2]}-${parts[1].padStart(2, '0')}-${parts[0].padStart(2, '0')}`;
  }

  async saveModel() {
  if (!this.model) {
    throw new Error('No model to save');
  }

  // Extract model topology (config) v√† weights
  const modelTopology = this.model.toJSON(); // Tr·∫£ v·ªÅ object config c·ªßa model
  const weightSpecs = this.model.weights.map(w => w.read().dataSync()); // Extract weights as arrays

  // Convert weights th√†nh d·∫°ng l∆∞u ƒë∆∞·ª£c (JSON stringifiable)
  const weightData = weightSpecs.map(ws => Array.from(ws)); // Chuy·ªÉn DataSync() th√†nh array

  const modelInfo = {
    modelName: NN_MODEL_NAME,
    inputNodes: this.inputNodes,
    topology: modelTopology, // L∆∞u config
    weights: weightData,     // L∆∞u weights arrays
    version: MODEL_VERSION,  // Th√™m version ƒë·ªÉ check khi load
    savedAt: new Date().toISOString()
  };

  // L∆∞u v√†o DB (NNState)
  await NNState.findOneAndUpdate(
    { modelName: NN_MODEL_NAME },
    { state: modelInfo }, // L∆∞u to√†n b·ªô info v√†o state
    { upsert: true }
  );

  console.log(`üíæ TensorFlow model saved to DB v·ªõi ${this.inputNodes} input nodes`);
}

async loadModel() {
    const modelState = await NNState.findOne({ modelName: NN_MODEL_NAME });
    if (modelState && modelState.state && modelState.state.topology && modelState.state.weights) {
      // Check version ƒë·ªÉ tr√°nh load model c≈© v·ªõi features m·ªõi
      if (modelState.state.version !== MODEL_VERSION) {
        console.warn(`‚ùå Model version mismatch: expected ${MODEL_VERSION}, got ${modelState.state.version}. Will rebuild.`);
        return false;
      }

      // Rebuild model t·ª´ topology
      this.model = tf.models.modelFromJSON(modelState.state.topology);

      // Set weights
      const weightTensors = modelState.state.weights.map(w => tf.tensor(w));
      this.model.setWeights(weightTensors);

      this.inputNodes = modelState.state.inputNodes;
      console.log(`‚úÖ TensorFlow model loaded t·ª´ DB v·ªõi ${this.inputNodes} input nodes`);
      return true;
    }
    return false;
  }

  async runHistoricalTraining() {
    console.log('üîî [TensorFlow Service] Starting Historical Training...');
    
    const trainingData = await this.prepareTrainingData();
    if (trainingData.length === 0) {
      throw new Error('Kh√¥ng c√≥ d·ªØ li·ªáu training');
    }

    const inputs = trainingData.map(d => d.inputSequence);
    const targets = trainingData.map(d => d.targetArray);

    await this.buildModel(this.inputNodes);
    await this.trainModel({ inputs, targets });
    await this.saveModel();

    return {
      message: `TensorFlow LSTM training completed. ${trainingData.length} sequences, ${EPOCHS} epochs.`,
      sequences: trainingData.length,
      epochs: EPOCHS,
      featureSize: this.inputNodes
    };
  }

  async runNextDayPrediction() {
    console.log('üîî [TensorFlow Service] Generating next day prediction...');
    
    if (!this.model) {
      const modelLoaded = await this.loadModel();
      if (!modelLoaded) {
        throw new Error('Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y ch·∫°y hu·∫•n luy·ªán tr∆∞·ªõc.');
      }
    }

    const results = await Result.find().lean();
    if (results.length < 1) { // Kh√¥ng y√™u c·∫ßu ƒë·ªß SEQUENCE_LENGTH n·ªØa, v√¨ s·∫Ω pad
      throw new Error('Kh√¥ng c√≥ d·ªØ li·ªáu.');
    }

    const grouped = {};
    results.forEach(r => {
      if (!grouped[r.ngay]) grouped[r.ngay] = [];
      grouped[r.ngay].push(r);
    });

    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    let latestSequenceDays = days.slice(-SEQUENCE_LENGTH);

    // N·∫øu kh√¥ng ƒë·ªß sequence, pad v·ªõi ng√†y gi·∫£ (features zeros)
    const paddingDay = Array(this.inputNodes).fill(0); // Pad v·ªõi zeros
    while (latestSequenceDays.length < SEQUENCE_LENGTH) {
      latestSequenceDays.unshift('padding'); // Th√™m padding ·ªü ƒë·∫ßu
    }

    const previousDays = [];
    const inputSequence = latestSequenceDays.map((day, index) => {
      if (day === 'padding') {
        return paddingDay; // S·ª≠ d·ª•ng padding zeros cho ng√†y gi·∫£
      }
      const dayResults = grouped[day] || [];
      const prevDays = previousDays.slice();
      previousDays.push(dayResults);
      return this.featureService.extractAllFeatures(dayResults, prevDays, day);
    });

    const output = await this.predict(inputSequence);
    const prediction = this.decodeOutput(output);

    const latestDay = days[days.length - 1]; // L·∫•y ng√†y th·∫≠t cu·ªëi c√πng
    const nextDayStr = DateTime.fromFormat(latestDay, 'dd/MM/yyyy').plus({ days: 1 }).toFormat('dd/MM/yyyy');

    await NNPrediction.findOneAndUpdate(
      { ngayDuDoan: nextDayStr },
      { ngayDuDoan: nextDayStr, ...prediction, danhDauDaSo: false },
      { upsert: true, new: true }
    );

    return {
      message: `TensorFlow LSTM ƒë√£ t·∫°o d·ª± ƒëo√°n cho ng√†y ${nextDayStr}.`,
      ngayDuDoan: nextDayStr
    };
  }

  // TH√äM H√ÄM runLearning ƒê·∫¶Y ƒê·ª¶
  async runLearning() {
    console.log('üîî [TensorFlow Service] Learning from new results...');
    
    if (!this.model) {
      const modelLoaded = await this.loadModel();
      if (!modelLoaded) {
        throw new Error('Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y ch·∫°y hu·∫•n luy·ªán tr∆∞·ªõc.');
      }
    }

    const predictionsToLearn = await NNPrediction.find({ danhDauDaSo: false }).lean();
    if (!predictionsToLearn.length) {
      return { message: 'Kh√¥ng c√≥ d·ª± ƒëo√°n m·ªõi n√†o ƒë·ªÉ h·ªçc.' };
    }

    const allResults = await Result.find().sort({ 'ngay': 1 }).lean();
    const grouped = {};
    allResults.forEach(r => {
      if (!grouped[r.ngay]) grouped[r.ngay] = [];
      grouped[r.ngay].push(r);
    });
    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    
    let learnedCount = 0;
    const trainingData = [];

    for (const pred of predictionsToLearn) {
      const targetDayStr = pred.ngayDuDoan;
      const targetDayIndex = days.indexOf(targetDayStr);

      if (targetDayIndex >= SEQUENCE_LENGTH) {
        const actualResult = (grouped[targetDayStr] || []).find(r => r.giai === 'ƒêB');
        
        if (actualResult?.so && String(actualResult.so).length >= 5) {
          const sequenceDays = days.slice(targetDayIndex - SEQUENCE_LENGTH, targetDayIndex);
          
          const previousDays = [];
          const inputSequence = sequenceDays.map(day => {
            const dayResults = grouped[day] || [];
            const prevDays = previousDays.slice();
            previousDays.push(dayResults);
            return this.featureService.extractAllFeatures(dayResults, prevDays, day);
          });

          const targetGDBString = String(actualResult.so).padStart(5, '0');
          const targetArray = this.prepareTarget(targetGDBString);
          
          trainingData.push({ inputSequence, targetArray });
          learnedCount++;
        }
      }
      
      await NNPrediction.updateOne({ _id: pred._id }, { danhDauDaSo: true });
    }

    if (trainingData.length > 0) {
      const inputs = trainingData.map(d => d.inputSequence);
      const targets = trainingData.map(d => d.targetArray);

      const inputTensor = tf.tensor3d(inputs, [inputs.length, SEQUENCE_LENGTH, this.inputNodes]);
      const targetTensor = tf.tensor2d(targets, [targets.length, OUTPUT_NODES]);

      await this.model.fit(inputTensor, targetTensor, {
        epochs: 10,
        batchSize: Math.min(8, trainingData.length),
        validationSplit: 0.2,
        callbacks: {
          onEpochEnd: (epoch, logs) => {
            console.log(`Fine-tuning Epoch ${epoch + 1}: Loss = ${logs.loss?.toFixed(4) || 'N/A'}`);
          }
        }
      });

      inputTensor.dispose();
      targetTensor.dispose();
      
      await this.saveModel();
    }
    
    return { 
      message: `TensorFlow LSTM ƒë√£ h·ªçc t·ª´ ${learnedCount} k·∫øt qu·∫£ m·ªõi.`,
      learnedCount 
    };
  }

  decodeOutput(output) {
    const prediction = { pos1: [], pos2: [], pos3: [], pos4: [], pos5: [] };
    for (let i = 0; i < 5; i++) {
      const positionOutput = output.slice(i * 10, (i + 1) * 10);
      const digitsWithValues = positionOutput
        .map((value, index) => ({ digit: String(index), value }))
        .sort((a, b) => b.value - a.value)
        .slice(0, 5)
        .map(item => item.digit);
      prediction[`pos${i + 1}`] = digitsWithValues;
    }
    return prediction;
  }
}

module.exports = TensorFlowService;
