const tf = require('@tensorflow/tfjs-node');
const Result = require('../models/Result');
const NNPrediction = require('../models/NNPrediction');
const NNState = require('../models/NNState');
const AdvancedFeatureEngineer = require('./advancedFeatureService');
const FeatureEngineeringService = require('./featureEngineeringService');

const { DateTime } = require('luxon');


const NN_MODEL_NAME = 'GDB_LSTM_TFJS_PREMIUM_V1'; // ƒê·ªïi t√™n model ƒë·ªÉ l∆∞u tr·∫°ng th√°i m·ªõi
const SEQUENCE_LENGTH = 7;
const OUTPUT_NODES = 50;
const EPOCHS = 50; // C√≥ th·ªÉ tƒÉng l√™n 70-100 v·ªõi model ph·ª©c t·∫°p h∆°n
const BATCH_SIZE = 32;

class TensorFlowService {
  constructor() {
    this.model = null;
    this.featureService = new FeatureEngineeringService();
    this.advancedFeatureEngineer = new AdvancedFeatureEngineer();
    this.inputNodes = 0; 
  }

  async buildModel(inputNodes) {
    console.log(`üèóÔ∏è B·∫Øt ƒë·∫ßu x√¢y d·ª±ng ki·∫øn tr√∫c Premium Model v·ªõi ${inputNodes} features...`);
    this.inputNodes = inputNodes;

    const model = tf.sequential();

    // --- T·∫¶NG 1: L·ªöP LSTM CH√çNH ---
    // Nhi·ªám v·ª•: X·ª≠ l√Ω tr·ª±c ti·∫øp chu·ªói 7 ng√†y x 346 features. L·ªõp n√†y h·ªçc c√°c m·∫´u h√¨nh th·ªùi gian (temporal patterns) ·ªü m·ª©c ƒë·ªô th·∫•p.
    model.add(tf.layers.lstm({
      units: 192,                         // S·ªë l∆∞·ª£ng n∆°-ron (b·ªô nh·ªõ) trong l·ªõp LSTM. 192 l√† m·ªôt con s·ªë l·ªõn, ph√π h·ª£p v·ªõi l∆∞·ª£ng features ƒë·∫ßu v√†o cao.
      returnSequences: true,              // R·∫•t QUAN TR·ªåNG. ƒê·∫∑t l√† `true` ƒë·ªÉ output c·ªßa l·ªõp n√†y v·∫´n l√† m·ªôt chu·ªói (sequence), l√†m ƒë·∫ßu v√†o cho l·ªõp LSTM ti·∫øp theo.
      inputShape: [SEQUENCE_LENGTH, inputNodes], // ƒê·ªãnh nghƒ©a h√¨nh d·∫°ng ƒë·∫ßu v√†o: 7 b∆∞·ªõc th·ªùi gian, m·ªói b∆∞·ªõc c√≥ `inputNodes` features.
      kernelRegularizer: tf.regularizers.l2({l2: 0.001}), // K·ªπ thu·∫≠t ch√≠nh quy h√≥a L2: "Ph·∫°t" c√°c tr·ªçng s·ªë (weights) c√≥ gi√° tr·ªã qu√° l·ªõn, bu·ªôc m√¥ h√¨nh ph·∫£i h·ªçc c√°c m·∫´u h√¨nh t·ªïng qu√°t h∆°n thay v√¨ d·ª±a d·∫´m v√†o m·ªôt v√†i features. Gi√∫p ch·ªëng overfitting.
      recurrentRegularizer: tf.regularizers.l2({l2: 0.001}) // T∆∞∆°ng t·ª± L2 nh∆∞ng √°p d·ª•ng cho c√°c tr·ªçng s·ªë k·∫øt n·ªëi n·ªôi b·ªô (recurrent connections) c·ªßa LSTM.
    }));

    // --- L·ªöP ·ªîN ƒê·ªäNH H√ìA ---
    // Nhi·ªám v·ª•: Chu·∫©n h√≥a output c·ªßa l·ªõp LSTM tr√™n, gi√∫p qu√° tr√¨nh h·ªçc ·ªü c√°c l·ªõp sau di·ªÖn ra nhanh v√† ·ªïn ƒë·ªãnh h∆°n.
    model.add(tf.layers.batchNormalization());

    // --- L·ªöP LO·∫†I B·ªé (DROPOUT) ---
    // Nhi·ªám v·ª•: Ch·ªëng overfitting. Trong m·ªói l∆∞·ª£t training, n√≥ s·∫Ω ng·∫´u nhi√™n "t·∫Øt" 25% c√°c n∆°-ron, bu·ªôc c√°c n∆°-ron c√≤n l·∫°i ph·∫£i h·ªçc m·ªôt c√°ch ƒë·ªôc l·∫≠p v√† m·∫°nh m·∫Ω h∆°n.
    model.add(tf.layers.dropout({rate: 0.25}));

    // --- T·∫¶NG 2: L·ªöP LSTM TH·ª® HAI ---
    // Nhi·ªám v·ª•: Nh·∫≠n chu·ªói output t·ª´ t·∫ßng 1 v√† h·ªçc c√°c m·∫´u h√¨nh ·ªü m·ª©c cao h∆°n ("m·∫´u h√¨nh c·ªßa c√°c m·∫´u h√¨nh").
    model.add(tf.layers.lstm({
      units: 96,                          // S·ªë units c√≥ th·ªÉ gi·∫£m d·∫ßn ·ªü c√°c l·ªõp sau v√¨ th√¥ng tin ƒë√£ ƒë∆∞·ª£c tr·ª´u t∆∞·ª£ng h√≥a.
      returnSequences: false,             // QUAN TR·ªåNG. ƒê·∫∑t l√† `false` v√¨ ƒë√¢y l√† l·ªõp LSTM cu·ªëi c√πng. Output c·ªßa n√≥ s·∫Ω l√† m·ªôt vector duy nh·∫•t (k√≠ch th∆∞·ªõc 96) ƒë·∫°i di·ªán cho to√†n b·ªô chu·ªói, s·∫µn s√†ng ƒë·ªÉ ƒë∆∞a v√†o c√°c l·ªõp Dense.
      kernelRegularizer: tf.regularizers.l2({l2: 0.001}),
      recurrentRegularizer: tf.regularizers.l2({l2: 0.001})
    }));

    model.add(tf.layers.batchNormalization());
    model.add(tf.layers.dropout({rate: 0.25}));
    
    // --- T·∫¶NG 3: L·ªöP K·∫æT N·ªêI ƒê·∫¶Y ƒê·ª¶ (DENSE) ---
    // Nhi·ªám v·ª•: Ho·∫°t ƒë·ªông nh∆∞ m·ªôt l·ªõp ph√¢n lo·∫°i cu·ªëi c√πng, k·∫øt h·ª£p c√°c features b·∫≠c cao ƒë√£ ƒë∆∞·ª£c h·ªçc b·ªüi c√°c l·ªõp LSTM ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh.
    model.add(tf.layers.dense({
      units: 48,
      activation: 'relu',                 // H√†m k√≠ch ho·∫°t 'relu' (Rectified Linear Unit) r·∫•t ph·ªï bi·∫øn v√† hi·ªáu qu·∫£, gi√∫p m√¥ h√¨nh h·ªçc c√°c m·ªëi quan h·ªá phi tuy·∫øn.
      kernelRegularizer: tf.regularizers.l2({l2: 0.001})
    }));

    // --- T·∫¶NG 4: L·ªöP OUTPUT CU·ªêI C√ôNG ---
    // Nhi·ªám v·ª•: ƒê∆∞a ra d·ª± ƒëo√°n cu·ªëi c√πng.
    model.add(tf.layers.dense({
      units: OUTPUT_NODES,                // 50 units (5 v·ªã tr√≠ * 10 ch·ªØ s·ªë).
      activation: 'sigmoid'               // H√†m k√≠ch ho·∫°t 'sigmoid' √©p c√°c gi√° tr·ªã output v·ªÅ kho·∫£ng [0, 1]. R·∫•t ph√π h·ª£p cho b√†i to√°n ph√¢n lo·∫°i ƒëa nh√£n (multi-label classification) n√†y, v√¨ m·ªói output ƒë·∫°i di·ªán cho "x√°c su·∫•t" m·ªôt ch·ªØ s·ªë xu·∫•t hi·ªán ·ªü m·ªôt v·ªã tr√≠.
    }));
    
    // In ra c·∫•u tr√∫c c·ªßa model ƒë·ªÉ ki·ªÉm tra.
    model.summary();

    this.model = model;
    return this.model;
  }

  async trainModel(trainingData) {
    const { inputs, targets } = trainingData;

    const inputTensor = tf.tensor3d(inputs, [inputs.length, SEQUENCE_LENGTH, this.inputNodes]);
    const targetTensor = tf.tensor2d(targets, [targets.length, OUTPUT_NODES]);

    const history = await this.model.fit(inputTensor, targetTensor, {
      epochs: EPOCHS,
      batchSize: BATCH_SIZE,
      validationSplit: 0.1,
      callbacks: {
        onEpochEnd: (epoch, logs) => {
          // Ch·ªâ in ra loss ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≥ l·ªói n√†o kh√°c x·∫£y ra.
          console.log(`Epoch ${epoch + 1}: Loss = ${logs.loss.toFixed(4)}`);
        }
      }
    });

    inputTensor.dispose();
    targetTensor.dispose();

    return history;
  }

  async predict(inputSequence) {
    const inputTensor = tf.tensor3d([inputSequence], [1, SEQUENCE_LENGTH, this.inputNodes]);
    const prediction = this.model.predict(inputTensor);
    const output = await prediction.data();
    prediction.dispose();
    inputTensor.dispose();
    return Array.from(output);
  }

  prepareTarget(gdbString) {
    const target = Array(OUTPUT_NODES).fill(0.01);
    gdbString.split('').forEach((digit, index) => {
      const d = parseInt(digit);
      if (!isNaN(d) && index < 5) {
        target[index * 10 + d] = 0.99;
      }
    });
    return target;
  }

  async prepareTrainingData() {
    console.log('üìù B·∫Øt ƒë·∫ßu chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán...');
    const results = await Result.find().sort({ 'ngay': 1 }).lean();
    if (results.length < SEQUENCE_LENGTH + 1) {
        throw new Error(`Kh√¥ng ƒë·ªß d·ªØ li·ªáu. C·∫ßn √≠t nh·∫•t ${SEQUENCE_LENGTH + 1} ng√†y.`);
    }

    const grouped = {};
    results.forEach(r => {
        if (!grouped[r.ngay]) grouped[r.ngay] = [];
        grouped[r.ngay].push(r);
    });

    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    const trainingData = [];

    for (let i = 0; i < days.length - SEQUENCE_LENGTH; i++) {
        const sequenceDaysStrings = days.slice(i, i + SEQUENCE_LENGTH);
        const targetDayString = days[i + SEQUENCE_LENGTH];
        
        const allHistoryForSequence = days.slice(0, i + SEQUENCE_LENGTH).map(dayStr => grouped[dayStr] || []);
        const inputSequence = [];
        
        let sequenceHasInvalidData = false; // C·ªù ƒë·ªÉ ki·ªÉm tra sequence hi·ªán t·∫°i

        for(let j = 0; j < SEQUENCE_LENGTH; j++) {
            const currentDayForFeature = grouped[sequenceDaysStrings[j]] || [];
            const dateStr = sequenceDaysStrings[j];
            
            const previousDaysForBasicFeatures = allHistoryForSequence.slice(0, i + j);
            const previousDaysForAdvancedFeatures = previousDaysForBasicFeatures.slice().reverse();

            const basicFeatures = this.featureService.extractAllFeatures(currentDayForFeature, previousDaysForBasicFeatures, dateStr);
            const advancedFeatures = this.advancedFeatureEngineer.extractPremiumFeatures(currentDayForFeature, previousDaysForAdvancedFeatures);
            
            let finalFeatureVector = [...basicFeatures, ...advancedFeatures];

            // =================================================================
            // ƒê√ÇY L√Ä "T·∫§M L√Å CH·∫ÆN" M·ªöI - B∆Ø·ªöC KI·ªÇM TRA V√Ä L√ÄM S·∫†CH
            // =================================================================
            const initialLength = finalFeatureVector.length;
            finalFeatureVector = finalFeatureVector.map(val => {
                // Ki·ªÉm tra xem gi√° tr·ªã c√≥ ph·∫£i l√† null, undefined, ho·∫∑c NaN kh√¥ng.
                if (val === null || val === undefined || isNaN(val)) {
                    // N·∫øu kh√¥ng h·ª£p l·ªá, ghi l·∫°i c·∫£nh b√°o v√† thay th·∫ø b·∫±ng 0.
                    // Vi·ªác n√†y gi√∫p ch∆∞∆°ng tr√¨nh kh√¥ng b·ªã s·∫≠p v√† ta c√≥ th·ªÉ ƒëi·ªÅu tra sau.
                    if (!sequenceHasInvalidData) { // Ch·ªâ log 1 l·∫ßn cho m·ªói sequence b·ªã l·ªói
                        console.warn(`
                            ‚ö†Ô∏è C·∫¢NH B√ÅO: Ph√°t hi·ªán d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá trong chu·ªói b·∫Øt ƒë·∫ßu t·ª´ ng√†y ${sequenceDaysStrings[0]}.
                            Ng√†y c·ª• th·ªÉ c√≥ v·∫•n ƒë·ªÅ: ${dateStr}.
                            Gi√° tr·ªã kh√¥ng h·ª£p l·ªá ƒë√£ ƒë∆∞·ª£c thay th·∫ø b·∫±ng 0.
                            H√£y ki·ªÉm tra l·∫°i logic trong c√°c h√†m feature engineering cho ng√†y n√†y.
                        `);
                    }
                    sequenceHasInvalidData = true;
                    return 0; // Thay th·∫ø gi√° tr·ªã kh√¥ng h·ª£p l·ªá b·∫±ng 0.
                }
                return val;
            });
            // =================================================================

            inputSequence.push(finalFeatureVector);
        }

        const targetGDB = (grouped[targetDayString] || []).find(r => r.giai === 'ƒêB');
        if (targetGDB?.so && String(targetGDB.so).length >= 5) {
            const targetGDBString = String(targetGDB.so).padStart(5, '0');
            const targetArray = this.prepareTarget(targetGDBString);

            // B∆Ø·ªöC KI·ªÇM TRA B·ªî SUNG CHO M·∫¢NG TARGETS
            if (targetArray.some(val => val === null || val === undefined || isNaN(val))) {
                console.error(`
                    ‚ùå L·ªñI NGHI√äM TR·ªåNG: M·∫£ng target cho ng√†y ${targetDayString} ch·ª©a gi√° tr·ªã kh√¥ng h·ª£p l·ªá.
                    B·ªè qua chu·ªói n√†y. Vui l√≤ng ki·ªÉm tra h√†m prepareTarget.
                `);
                continue; // B·ªè qua, kh√¥ng th√™m chu·ªói n√†y v√†o trainingData
            }

            trainingData.push({ inputSequence, targetArray });
        }
    }

    if (trainingData.length > 0) {
        this.inputNodes = trainingData[0].inputSequence[0].length;
        console.log(`‚úÖ ƒê√£ chu·∫©n b·ªã ${trainingData.length} chu·ªói d·ªØ li·ªáu hu·∫•n luy·ªán h·ª£p l·ªá v·ªõi feature size: ${this.inputNodes}`);
    } else {
        console.error("‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ t·∫°o ƒë∆∞·ª£c b·∫•t k·ª≥ chu·ªói d·ªØ li·ªáu hu·∫•n luy·ªán n√†o. Vui l√≤ng ki·ªÉm tra l·∫°i to√†n b·ªô d·ªØ li·ªáu ngu·ªìn v√† logic `prepareTrainingData`.");
        throw new Error("Kh√¥ng c√≥ d·ªØ li·ªáu training h·ª£p l·ªá.");
    }

    return trainingData;
}

  dateKey(s) {
    if (!s || typeof s !== 'string') return '';
    const parts = s.split('/');
    return parts.length !== 3 ? s : `${parts[2]}-${parts[1].padStart(2, '0')}-${parts[0].padStart(2, '0')}`;
  }

  async saveModel() {
    if (!this.model) {
      throw new Error('No model to save');
    }

    const modelInfo = {
      modelName: NN_MODEL_NAME,
      inputNodes: this.inputNodes,
      savedAt: new Date().toISOString()
    };

    // L∆∞u model d∆∞·ªõi d·∫°ng JSON (c√≥ th·ªÉ l∆∞u v√†o file ho·∫∑c database)
    const saveResult = await this.model.save('file://./models/tfjs_model');
    
    await NNState.findOneAndUpdate(
      { modelName: NN_MODEL_NAME },
      { 
        state: modelInfo,
        modelArtifacts: saveResult 
      },
      { upsert: true }
    );

    console.log(`üíæ TensorFlow model saved v·ªõi ${this.inputNodes} input nodes`);
  }

  async loadModel() {
    const modelState = await NNState.findOne({ modelName: NN_MODEL_NAME });
    if (modelState && modelState.modelArtifacts) {
      this.model = await tf.loadLayersModel('file://./models/tfjs_model/model.json');
      this.inputNodes = modelState.state.inputNodes;
      console.log(`‚úÖ TensorFlow model loaded v·ªõi ${this.inputNodes} input nodes`);
      return true;
    }
    return false;
  }

  // =================================================================
  // C·∫¨P NH·∫¨T H√ÄM runHistoricalTraining ƒê·ªÇ S·ª¨ D·ª§NG MODEL M·ªöI
  // =================================================================
  async runHistoricalTraining() {
    console.log('üîî [TensorFlow Service] B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán L·ªãch s·ª≠ v·ªõi ki·∫øn tr√∫c Premium...');
    
    const trainingData = await this.prepareTrainingData(); // H√†m n√†y ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ·ªü B∆∞·ªõc 1
    if (trainingData.length === 0) {
      throw new Error('Kh√¥ng c√≥ d·ªØ li·ªáu training');
    }

    const inputs = trainingData.map(d => d.inputSequence);
    const targets = trainingData.map(d => d.targetArray);

    // X√¢y d·ª±ng model m·ªõi d·ª±a tr√™n s·ªë features th·ª±c t·∫ø
    // this.inputNodes ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t trong `prepareTrainingData`
    this.buildModel(this.inputNodes); 

    // COMPILE MODEL: C·∫•u h√¨nh qu√° tr√¨nh h·ªçc
    this.model.compile({
      optimizer: tf.train.adam({learningRate: 0.0005}),
      loss: 'binaryCrossentropy'
      // T·∫†M TH·ªúI LO·∫†I B·ªé HO√ÄN TO√ÄN 'metrics'.
      // Qu√° tr√¨nh h·ªçc c·ªßa m√¥ h√¨nh d·ª±a tr√™n 'loss', n√™n v·∫´n s·∫Ω ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng.
      // Ch√∫ng ta s·∫Ω ch·ªâ m·∫•t ƒëi ph·∫ßn hi·ªÉn th·ªã accuracy/precision trong log c·ªßa m·ªói epoch.
    });
    console.log('‚úÖ Model ƒë√£ ƒë∆∞·ª£c compile. B·∫Øt ƒë·∫ßu qu√° tr√¨nh training...');

    // Hu·∫•n luy·ªán model
    await this.trainModel({ inputs, targets }); 
    
    // L∆∞u model sau khi hu·∫•n luy·ªán xong
    await this.saveModel(); 

    return {
      message: `Hu·∫•n luy·ªán Premium Model ho√†n t·∫•t. ƒê√£ x·ª≠ l√Ω ${trainingData.length} chu·ªói, ${EPOCHS} epochs.`,
      sequences: trainingData.length,
      epochs: EPOCHS,
      featureSize: this.inputNodes,
      modelName: NN_MODEL_NAME
    };
  }

  async runLearning() {
  console.log('üîî [TensorFlow Service] Learning from new results...');
  
  if (!this.model) {
    const modelLoaded = await this.loadModel();
    if (!modelLoaded) {
      throw new Error('Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y ch·∫°y hu·∫•n luy·ªán l·ªãch s·ª≠ tr∆∞·ªõc.');
    }
  }

  // L·∫•y c√°c d·ª± ƒëo√°n ch∆∞a ƒë∆∞·ª£c h·ªçc
  const predictionsToLearn = await NNPrediction.find({ danhDauDaSo: false }).lean();
  if (predictionsToLearn.length === 0) {
    return { message: 'Kh√¥ng c√≥ d·ª± ƒëo√°n m·ªõi n√†o ƒë·ªÉ h·ªçc.' };
  }

  const results = await Result.find().sort({ 'ngay': 1 }).lean();
  const grouped = {};
  results.forEach(r => {
    if (!grouped[r.ngay]) grouped[r.ngay] = [];
    grouped[r.ngay].push(r);
  });

  const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
  
  let learnedCount = 0;
  const trainingData = [];

  for (const pred of predictionsToLearn) {
    const targetDayStr = pred.ngayDuDoan;
    const targetDayIndex = days.indexOf(targetDayStr);

    if (targetDayIndex >= SEQUENCE_LENGTH) {
      const actualResult = (grouped[targetDayStr] || []).find(r => r.giai === 'ƒêB');
      
      if (actualResult?.so && String(actualResult.so).length >= 5) {
        // L·∫•y chu·ªói input
        const sequenceDays = days.slice(targetDayIndex - SEQUENCE_LENGTH, targetDayIndex);
        const previousDays = [];
        const inputSequence = sequenceDays.map(day => {
          const dayResults = grouped[day] || [];
          const prevDays = previousDays.slice();
          previousDays.push(dayResults);
          return this.featureService.extractAllFeatures(dayResults, prevDays, day);
        });

        // L·∫•y target
        const targetGDBString = String(actualResult.so).padStart(5, '0');
        const targetArray = this.prepareTarget(targetGDBString);
        
        trainingData.push({ inputSequence, targetArray });
        learnedCount++;
      }
    }
    // ƒê√°nh d·∫•u ƒë√£ h·ªçc
    await NNPrediction.updateOne({ _id: pred._id }, { danhDauDaSo: true });
  }

  if (trainingData.length > 0) {
    const inputs = trainingData.map(d => d.inputSequence);
    const targets = trainingData.map(d => d.targetArray);

    // Hu·∫•n luy·ªán th√™m v·ªõi d·ªØ li·ªáu m·ªõi
    const inputTensor = tf.tensor3d(inputs, [inputs.length, SEQUENCE_LENGTH, this.inputNodes]);
    const targetTensor = tf.tensor2d(targets, [targets.length, OUTPUT_NODES]);

    await this.model.fit(inputTensor, targetTensor, {
      epochs: 3, // S·ªë epoch √≠t h∆°n ƒë·ªÉ h·ªçc nhanh
      batchSize: Math.min(BATCH_SIZE, inputs.length),
      validationSplit: 0.1
    });

    inputTensor.dispose();
    targetTensor.dispose();

    await this.saveModel();
  }
  
  return { message: `TensorFlow LSTM ƒë√£ h·ªçc xong. ƒê√£ x·ª≠ l√Ω ${learnedCount} k·∫øt qu·∫£ m·ªõi.` };
}

  async runNextDayPrediction() {
    console.log('üîî [TensorFlow Service] Generating next day prediction...');
    
    if (!this.model) {
      const modelLoaded = await this.loadModel();
      if (!modelLoaded) {
        throw new Error('Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y ch·∫°y hu·∫•n luy·ªán tr∆∞·ªõc.');
      }
    }

    const results = await Result.find().lean();
    if (results.length < SEQUENCE_LENGTH) {
      throw new Error(`Kh√¥ng ƒë·ªß d·ªØ li·ªáu. C·∫ßn √≠t nh·∫•t ${SEQUENCE_LENGTH} ng√†y.`);
    }

    const grouped = {};
    results.forEach(r => {
      if (!grouped[r.ngay]) grouped[r.ngay] = [];
      grouped[r.ngay].push(r);
    });

    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    const latestSequenceDays = days.slice(-SEQUENCE_LENGTH);

    const previousDays = [];
    const inputSequence = latestSequenceDays.map(day => {
      const dayResults = grouped[day] || [];
      const prevDays = previousDays.slice();
      previousDays.push(dayResults);
      return this.featureService.extractAllFeatures(dayResults, prevDays, day);
    });

    const output = await this.predict(inputSequence);
    const prediction = this.decodeOutput(output);

    const latestDay = latestSequenceDays[latestSequenceDays.length - 1];
    const nextDayStr = DateTime.fromFormat(latestDay, 'dd/MM/yyyy').plus({ days: 1 }).toFormat('dd/MM/yyyy');

    await NNPrediction.findOneAndUpdate(
      { ngayDuDoan: nextDayStr },
      { ngayDuDoan: nextDayStr, ...prediction, danhDauDaSo: false },
      { upsert: true, new: true }
    );

    return {
      message: `TensorFlow LSTM ƒë√£ t·∫°o d·ª± ƒëo√°n cho ng√†y ${nextDayStr}.`,
      ngayDuDoan: nextDayStr
    };
  }

  decodeOutput(output) {
    const prediction = { pos1: [], pos2: [], pos3: [], pos4: [], pos5: [] };
    for (let i = 0; i < 5; i++) {
      const positionOutput = output.slice(i * 10, (i + 1) * 10);
      const digitsWithValues = positionOutput
        .map((value, index) => ({ digit: String(index), value }))
        .sort((a, b) => b.value - a.value)
        .slice(0, 5)
        .map(item => item.digit);
      prediction[`pos${i + 1}`] = digitsWithValues;
    }
    return prediction;
  }
}

module.exports = TensorFlowService;
