const tf = require('@tensorflow/tfjs-node');
const Result = require('../models/Result');
const NNPrediction = require('../models/NNPrediction');
const NNState = require('../models/NNState');
const AdvancedFeatureEngineer = require('./advancedFeatureService');
const FeatureEngineeringService = require('./featureEngineeringService');
const { DateTime } = require('luxon');
const NN_MODEL_NAME = 'GDB_LSTM_TFJS_PREMIUM_V1'; // ƒê·ªïi t√™n model ƒë·ªÉ l∆∞u tr·∫°ng th√°i m·ªõi
const SEQUENCE_LENGTH = 7;
const OUTPUT_NODES = 50;
const EPOCHS = 50; // C√≥ th·ªÉ tƒÉng l√™n 70-100 v·ªõi model ph·ª©c t·∫°p h∆°n
const BATCH_SIZE = 32;
class TensorFlowService {
  constructor() {
    this.model = null;
    this.featureService = new FeatureEngineeringService();
    this.advancedFeatureEngineer = new AdvancedFeatureEngineer();
    this.inputNodes = 0;
  }
  async buildModel(inputNodes) {
    console.log(`üèóÔ∏è B·∫Øt ƒë·∫ßu x√¢y d·ª±ng ki·∫øn tr√∫c Premium Model v·ªõi ${inputNodes} features...`);
    this.inputNodes = inputNodes;
    const model = tf.sequential();
    model.add(tf.layers.lstm({
      units: 192,
      returnSequences: true,
      inputShape: [SEQUENCE_LENGTH, inputNodes],
      kernelRegularizer: tf.regularizers.l2({l2: 0.001}),
      recurrentRegularizer: tf.regularizers.l2({l2: 0.001})
    }));
    model.add(tf.layers.batchNormalization({epsilon: 1e-5})); // TƒÉng epsilon tr√°nh div 0
    model.add(tf.layers.dropout({rate: 0.25}));
    model.add(tf.layers.lstm({
      units: 96,
      returnSequences: false,
      kernelRegularizer: tf.regularizers.l2({l2: 0.001}),
      recurrentRegularizer: tf.regularizers.l2({l2: 0.001})
    }));
    model.add(tf.layers.batchNormalization({epsilon: 1e-5}));
    model.add(tf.layers.dropout({rate: 0.25}));
    model.add(tf.layers.dense({
      units: 48,
      activation: 'relu',
      kernelRegularizer: tf.regularizers.l2({l2: 0.001})
    }));
    model.add(tf.layers.dense({
      units: OUTPUT_NODES,
      activation: 'sigmoid'
    }));
    model.summary();
    this.model = model;
    return model;
  }
  async trainModel(trainingData) {
    const { inputs, targets } = trainingData;
    if (!inputs || !targets || inputs.length === 0 || targets.length === 0) {
      throw new Error('D·ªØ li·ªáu training r·ªóng ho·∫∑c kh√¥ng h·ª£p l·ªá');
    }
    // KI·ªÇM TRA T·ª™NG PH·∫¶N T·ª¨
    inputs.forEach((input, idx) => {
      if (!input || input.length !== SEQUENCE_LENGTH) {
        throw new Error(`Input t·∫°i index ${idx} kh√¥ng h·ª£p l·ªá: ${input}`);
      }
    });
    targets.forEach((target, idx) => {
      if (!target || target.length !== OUTPUT_NODES) {
        throw new Error(`Target t·∫°i index ${idx} kh√¥ng h·ª£p l·ªá: ${target}`);
      }
    });
    const inputTensor = tf.tensor3d(inputs, [inputs.length, SEQUENCE_LENGTH, this.inputNodes]);
    const targetTensor = tf.tensor2d(targets, [targets.length, OUTPUT_NODES]);
    if (inputTensor.shape.some(dim => dim === 0) || targetTensor.shape.some(dim => dim === 0)) {
      throw new Error('Tensor c√≥ shape kh√¥ng h·ª£p l·ªá');
    }
    // Debug min/max input
    const inputData = await inputTensor.data();
    const minInput = Math.min(...inputData);
    const maxInput = Math.max(...inputData);
    console.log(`Input data min/max: ${minInput} / ${maxInput}`);
    if (isNaN(minInput) || isNaN(maxInput)) {
      throw new Error('Input ch·ª©a NaN');
    }
    const validationSplit = inputs.length >= 100 ? 0.1 : 0; // Tr√°nh val empty
    const history = await this.model.fit(inputTensor, targetTensor, {
      epochs: EPOCHS,
      batchSize: BATCH_SIZE,
      validationSplit: validationSplit,
      callbacks: {
        onEpochEnd: (epoch, logs) => {
          if (isNaN(logs.loss)) {
            console.error('NaN loss detected! Stopping training.');
            this.model.stopTraining = true;
          }
          console.log(`Epoch ${epoch + 1}: Loss = ${logs.loss ? logs.loss.toFixed(4) : 'NaN'}`);
        }
      }
    });
    inputTensor.dispose();
    targetTensor.dispose();
    return history;
  }
  async predict(inputSequence) {
    const inputTensor = tf.tensor3d([inputSequence], [1, SEQUENCE_LENGTH, this.inputNodes]);
    const prediction = this.model.predict(inputTensor);
    const output = await prediction.data();
    prediction.dispose();
    inputTensor.dispose();
    return Array.from(output);
  }
  prepareTarget(gdbString) {
    const target = Array(OUTPUT_NODES).fill(0);
    gdbString.split('').forEach((digit, index) => {
      const d = parseInt(digit);
      if (!isNaN(d) && index < 5) {
        target[index * 10 + d] = 1; // ƒê·ªïi th√†nh 1/0 ƒë·ªÉ ·ªïn ƒë·ªãnh loss
      }
    });
    return target;
  }
  async prepareTrainingData() {
    console.log('üìù B·∫Øt ƒë·∫ßu chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán...');
    const results = await Result.find().sort({ 'ngay': 1 }).lean();
    if (results.length < SEQUENCE_LENGTH + 1) {
      throw new Error(`Kh√¥ng ƒë·ªß d·ªØ li·ªáu. C·∫ßn √≠t nh·∫•t ${SEQUENCE_LENGTH + 1} ng√†y.`);
    }
    const grouped = {};
    results.forEach(r => {
      if (!grouped[r.ngay]) grouped[r.ngay] = [];
      grouped[r.ngay].push(r);
    });
    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    const trainingData = [];
    let allFeatures = []; // ƒê·ªÉ normalize sau
    for (let i = 0; i < days.length - SEQUENCE_LENGTH; i++) {
      const sequenceDaysStrings = days.slice(i, i + SEQUENCE_LENGTH);
      const targetDayString = days[i + SEQUENCE_LENGTH);
      const allHistoryForSequence = days.slice(0, i + SEQUENCE_LENGTH).map(dayStr => grouped[dayStr] || []);
      const inputSequence = [];
      let sequenceHasInvalidData = false;
      for(let j = 0; j < SEQUENCE_LENGTH; j++) {
        const currentDayForFeature = grouped[sequenceDaysStrings[j]] || [];
        const dateStr = sequenceDaysStrings[j];
        const previousDaysForBasicFeatures = allHistoryForSequence.slice(0, i + j);
        const previousDaysForAdvancedFeatures = previousDaysForBasicFeatures.slice().reverse();
        const basicFeatures = this.featureService.extractAllFeatures(currentDayForFeature, previousDaysForBasicFeatures, dateStr);
        const advancedFeatures = this.advancedFeatureEngineer.extractPremiumFeatures(currentDayForFeature, previousDaysForAdvancedFeatures);
        let finalFeatureVector = [...basicFeatures, ...Object.values(advancedFeatures).flat()];
        if (finalFeatureVector.some(isNaN) || finalFeatureVector.some(val => val === null || val === undefined)) {
          console.warn(`‚ö†Ô∏è Gi√° tr·ªã kh√¥ng h·ª£p l·ªá cho ng√†y ${dateStr}. L√†m s·∫°ch...`);
          finalFeatureVector = finalFeatureVector.map(val => isNaN(val) || val == null ? 0 : val);
        }
        const EXPECTED_FEATURE_SIZE = 346;
        if (finalFeatureVector.length !== EXPECTED_FEATURE_SIZE) {
          console.warn(`Sai size cho ng√†y ${dateStr}: ${finalFeatureVector.length}. ƒêi·ªÅu ch·ªânh...`);
          finalFeatureVector = finalFeatureVector.slice(0, EXPECTED_FEATURE_SIZE).concat(Array(Math.max(0, EXPECTED_FEATURE_SIZE - finalFeatureVector.length)).fill(0));
        }
        inputSequence.push(finalFeatureVector);
        allFeatures = allFeatures.concat(finalFeatureVector); // Thu th·∫≠p ƒë·ªÉ normalize
      }
      const targetGDB = (grouped[targetDayString] || []).find(r => r.giai === 'ƒêB');
      if (targetGDB?.so && String(targetGDB.so).length >= 5) {
        const targetGDBString = String(targetGDB.so).padStart(5, '0');
        const targetArray = this.prepareTarget(targetGDBString);
        if (targetArray.some(isNaN)) {
          console.error(`Target invalid cho ng√†y ${targetDayString}. B·ªè qua.`);
          continue;
        }
        trainingData.push({ inputSequence, targetArray });
      }
    }
    if (trainingData.length > 0) {
      // Normalize to√†n b·ªô features (min-max)
      const min = Math.min(...allFeatures);
      const max = Math.max(...allFeatures);
      console.log(`Normalizing features: min=${min}, max=${max}`);
      trainingData.forEach(d => {
        d.inputSequence = d.inputSequence.map(seq => seq.map(v => (v - min) / (max - min + 1e-8)));
      });
      this.inputNodes = trainingData[0].inputSequence[0].length;
      console.log(`‚úÖ ƒê√£ chu·∫©n b·ªã ${trainingData.length} chu·ªói v·ªõi feature size: ${this.inputNodes}`);
    } else {
      throw new Error("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu training h·ª£p l·ªá.");
    }
    return trainingData;
  }
  // C√°c h√†m c√≤n l·∫°i gi·ªØ nguy√™n...
  async runHistoricalTraining() {
    console.log('üîî [TensorFlow Service] B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán L·ªãch s·ª≠ v·ªõi ki·∫øn tr√∫c Premium...');
    const trainingData = await this.prepareTrainingData();
    if (trainingData.length === 0 || trainingData.some(d => d.inputSequence.length !== SEQUENCE_LENGTH || d.inputSequence.flat().some(isNaN))) {
      throw new Error('D·ªØ li·ªáu training r·ªóng ho·∫∑c invalid. Ki·ªÉm tra DB.');
    }
    const inputs = trainingData.map(d => d.inputSequence);
    const targets = trainingData.map(d => d.targetArray);
    this.buildModel(this.inputNodes);
    this.model.compile({
      optimizer: tf.train.adam({learningRate: 0.0005, clipnorm: 1.0}), // Clip gradients tr√°nh explosion
      loss: 'binaryCrossentropy',
      metrics: [] // B·ªè metrics t·∫°m n·∫øu g√¢y issue
    });
    console.log('‚úÖ Model compile OK. Training...');
    await this.trainModel({ inputs, targets });
    await this.saveModel();
    return {
      message: `Hu·∫•n luy·ªán OK: ${trainingData.length} sequences, ${EPOCHS} epochs.`,
      sequences: trainingData.length,
      epochs: EPOCHS,
      featureSize: this.inputNodes,
      modelName: NN_MODEL_NAME
    };
  }
  
  async runLearning() {
  console.log('üîî [TensorFlow Service] Learning from new results...');
  
  if (!this.model) {
    const modelLoaded = await this.loadModel();
    if (!modelLoaded) {
      throw new Error('Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y ch·∫°y hu·∫•n luy·ªán l·ªãch s·ª≠ tr∆∞·ªõc.');
    }
  }

  // L·∫•y c√°c d·ª± ƒëo√°n ch∆∞a ƒë∆∞·ª£c h·ªçc
  const predictionsToLearn = await NNPrediction.find({ danhDauDaSo: false }).lean();
  if (predictionsToLearn.length === 0) {
    return { message: 'Kh√¥ng c√≥ d·ª± ƒëo√°n m·ªõi n√†o ƒë·ªÉ h·ªçc.' };
  }

  const results = await Result.find().sort({ 'ngay': 1 }).lean();
  const grouped = {};
  results.forEach(r => {
    if (!grouped[r.ngay]) grouped[r.ngay] = [];
    grouped[r.ngay].push(r);
  });

  const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
  
  let learnedCount = 0;
  const trainingData = [];

  for (const pred of predictionsToLearn) {
    const targetDayStr = pred.ngayDuDoan;
    const targetDayIndex = days.indexOf(targetDayStr);

    if (targetDayIndex >= SEQUENCE_LENGTH) {
      const actualResult = (grouped[targetDayStr] || []).find(r => r.giai === 'ƒêB');
      
      if (actualResult?.so && String(actualResult.so).length >= 5) {
        // L·∫•y chu·ªói input
        const sequenceDays = days.slice(targetDayIndex - SEQUENCE_LENGTH, targetDayIndex);
        const previousDays = [];
        const inputSequence = sequenceDays.map(day => {
          const dayResults = grouped[day] || [];
          const prevDays = previousDays.slice();
          previousDays.push(dayResults);
          return this.featureService.extractAllFeatures(dayResults, prevDays, day);
        });

        // L·∫•y target
        const targetGDBString = String(actualResult.so).padStart(5, '0');
        const targetArray = this.prepareTarget(targetGDBString);
        
        trainingData.push({ inputSequence, targetArray });
        learnedCount++;
      }
    }
    // ƒê√°nh d·∫•u ƒë√£ h·ªçc
    await NNPrediction.updateOne({ _id: pred._id }, { danhDauDaSo: true });
  }

  if (trainingData.length > 0) {
    const inputs = trainingData.map(d => d.inputSequence);
    const targets = trainingData.map(d => d.targetArray);

    // Hu·∫•n luy·ªán th√™m v·ªõi d·ªØ li·ªáu m·ªõi
    const inputTensor = tf.tensor3d(inputs, [inputs.length, SEQUENCE_LENGTH, this.inputNodes]);
    const targetTensor = tf.tensor2d(targets, [targets.length, OUTPUT_NODES]);

    await this.model.fit(inputTensor, targetTensor, {
      epochs: 3, // S·ªë epoch √≠t h∆°n ƒë·ªÉ h·ªçc nhanh
      batchSize: Math.min(BATCH_SIZE, inputs.length),
      validationSplit: 0.1
    });

    inputTensor.dispose();
    targetTensor.dispose();

    await this.saveModel();
  }
  
  return { message: `TensorFlow LSTM ƒë√£ h·ªçc xong. ƒê√£ x·ª≠ l√Ω ${learnedCount} k·∫øt qu·∫£ m·ªõi.` };
}

  async runNextDayPrediction() {
    console.log('üîî [TensorFlow Service] Generating next day prediction...');
    
    if (!this.model) {
      const modelLoaded = await this.loadModel();
      if (!modelLoaded) {
        throw new Error('Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y ch·∫°y hu·∫•n luy·ªán tr∆∞·ªõc.');
      }
    }

    const results = await Result.find().lean();
    if (results.length < SEQUENCE_LENGTH) {
      throw new Error(`Kh√¥ng ƒë·ªß d·ªØ li·ªáu. C·∫ßn √≠t nh·∫•t ${SEQUENCE_LENGTH} ng√†y.`);
    }

    const grouped = {};
    results.forEach(r => {
      if (!grouped[r.ngay]) grouped[r.ngay] = [];
      grouped[r.ngay].push(r);
    });

    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    const latestSequenceDays = days.slice(-SEQUENCE_LENGTH);

    const previousDays = [];
    const inputSequence = latestSequenceDays.map(day => {
      const dayResults = grouped[day] || [];
      const prevDays = previousDays.slice();
      previousDays.push(dayResults);
      return this.featureService.extractAllFeatures(dayResults, prevDays, day);
    });

    const output = await this.predict(inputSequence);
    const prediction = this.decodeOutput(output);

    const latestDay = latestSequenceDays[latestSequenceDays.length - 1];
    const nextDayStr = DateTime.fromFormat(latestDay, 'dd/MM/yyyy').plus({ days: 1 }).toFormat('dd/MM/yyyy');

    await NNPrediction.findOneAndUpdate(
      { ngayDuDoan: nextDayStr },
      { ngayDuDoan: nextDayStr, ...prediction, danhDauDaSo: false },
      { upsert: true, new: true }
    );

    return {
      message: `TensorFlow LSTM ƒë√£ t·∫°o d·ª± ƒëo√°n cho ng√†y ${nextDayStr}.`,
      ngayDuDoan: nextDayStr
    };
  }

  decodeOutput(output) {
    const prediction = { pos1: [], pos2: [], pos3: [], pos4: [], pos5: [] };
    for (let i = 0; i < 5; i++) {
      const positionOutput = output.slice(i * 10, (i + 1) * 10);
      const digitsWithValues = positionOutput
        .map((value, index) => ({ digit: String(index), value }))
        .sort((a, b) => b.value - a.value)
        .slice(0, 5)
        .map(item => item.digit);
      prediction[`pos${i + 1}`] = digitsWithValues;
    }
    return prediction;
  }
}

module.exports = TensorFlowService;
