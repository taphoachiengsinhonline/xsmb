const tf = require('@tensorflow/tfjs-node');
const Result = require('../models/Result');
const NNPrediction = require('../models/NNPrediction');
const NNState = require('../models/NNState');
const AdvancedFeatureEngineer = require('./advancedFeatureService');
const FeatureEngineeringService = require('./featureEngineeringService');
const AdvancedTraining = require('./advancedTrainingService');
const { Storage } = require('@google-cloud/storage');
const { DateTime } = require('luxon');

// =================================================================
// C·∫§U H√åNH GCS - GI·ªÆ NGUY√äN
// =================================================================
const gcsCredentialsJSON = process.env.GCS_CREDENTIALS;
const bucketName = process.env.GCS_BUCKET_NAME;

let storage;
let bucket;

if (gcsCredentialsJSON && bucketName) {
    try {
        const credentials = JSON.parse(gcsCredentialsJSON);
        storage = new Storage({ credentials, projectId: credentials.project_id });
        bucket = storage.bucket(bucketName);
        console.log(`‚úÖ [GCS] ƒê√£ kh·ªüi t·∫°o Google Cloud Storage th√†nh c√¥ng cho bucket: ${bucketName}`);
    } catch (error) {
        console.error("‚ùå [GCS] L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ parse GCS_CREDENTIALS.", error);
        process.exit(1);
    }
} else {
    console.warn("‚ö†Ô∏è [GCS] C·∫£nh b√°o: GCS_CREDENTIALS ho·∫∑c GCS_BUCKET_NAME ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.");
}

const NN_MODEL_NAME = 'GDB_LSTM_TFJS_PREMIUM_V1';
const SEQUENCE_LENGTH = 7;
const OUTPUT_NODES = 50;
const EPOCHS = 100;
const BATCH_SIZE = 128;

const getGcsIoHandler = (modelPath) => {
    if (!bucket) {
        throw new Error("GCS Bucket ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.");
    }

    const modelJsonPath = `${modelPath}/model.json`;
    const weightsBinPath = `${modelPath}/weights.bin`;

    const handler = {
        save: async (modelArtifacts) => {
            console.log(`...[GCS IO] B·∫Øt ƒë·∫ßu upload model l√™n: ${modelPath}`);
            
            const weightsBuffer = Buffer.from(modelArtifacts.weightData);

            await Promise.all([
                bucket.file(modelJsonPath).save(JSON.stringify(modelArtifacts.modelTopology)),
                bucket.file(weightsBinPath).save(weightsBuffer)
            ]);

            console.log(`...[GCS IO] Upload th√†nh c√¥ng.`);
            return { modelArtifactsInfo: { dateSaved: new Date() } };
        },

        load: async () => {
            console.log(`...[GCS IO] B·∫Øt ƒë·∫ßu download model t·ª´: ${modelPath}`);

            const [modelJsonFile, weightsBinFile] = await Promise.all([
                bucket.file(modelJsonPath).download(),
                bucket.file(weightsBinPath).download()
            ]);

            const modelTopology = JSON.parse(modelJsonFile[0].toString());
            const weightData = weightsBinFile[0].buffer;

            console.log(`...[GCS IO] Download th√†nh c√¥ng.`);
            return { modelTopology, weightData };
        }
    };
    return handler;
};

class TensorFlowService {
  constructor() {
    this.model = null;
    this.featureService = new FeatureEngineeringService();
    this.advancedFeatureEngineer = new AdvancedFeatureEngineer();
    this.advancedTrainer = new AdvancedTraining();
    this.inputNodes = 0;
    this.ensembleModels = [];
    this.errorPatterns = null; // L∆∞u tr·ªØ ph√¢n t√≠ch l·ªói
  }

  // =================================================================
  // PH√ÇN T√çCH L·ªñI TO√ÄN DI·ªÜN - CH·∫†Y NGAY KHI C√ì D·ªÆ LI·ªÜU 90+ NG√ÄY
  // =================================================================
  async analyzeHistoricalErrors() {
    console.log('üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch l·ªói to√†n di·ªán t·ª´ 90+ ng√†y d·ªØ li·ªáu...');
    
    const results = await Result.find().sort({ 'ngay': 1 }).lean();
    const predictions = await NNPrediction.find().lean();

    if (results.length === 0 || predictions.length === 0) {
      console.log('‚ö†Ô∏è Ch∆∞a ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch l·ªói');
      return this.getDefaultErrorPatterns();
    }

    const groupedResults = {};
    results.forEach(r => {
      if (!groupedResults[r.ngay]) groupedResults[r.ngay] = [];
      groupedResults[r.ngay].push(r);
    });

    const errorAnalysis = {
      weakPositions: [],
      temporalWeaknesses: {},
      featureMistakes: new Set(),
      confidenceErrors: [],
      overallAccuracy: 0,
      totalAnalyzed: 0
    };

    let totalPredictions = 0;
    let correctPredictions = 0;

    // PH√ÇN T√çCH T·ª™NG D·ª∞ ƒêO√ÅN
    for (const pred of predictions) {
      const actual = (groupedResults[pred.ngayDuDoan] || []).find(r => r.giai === 'ƒêB');
      if (!actual?.so) continue;

      const actualStr = String(actual.so).padStart(5, '0');
      totalPredictions++;

      // KI·ªÇM TRA T·ª™NG V·ªä TR√ç
      let positionCorrect = true;
      for (let i = 0; i < 5; i++) {
        const predictedDigits = pred[`pos${i+1}`] || [];
        const actualDigit = actualStr[i];
        
        if (!predictedDigits.includes(actualDigit)) {
          errorAnalysis.weakPositions.push(`pos${i+1}`);
          positionCorrect = false;
        }
      }

      if (positionCorrect) correctPredictions++;

      // PH√ÇN T√çCH THEO TH·ªúI GIAN
      const date = DateTime.fromFormat(pred.ngayDuDoan, 'dd/MM/yyyy');
      const dayOfWeek = date.weekdayShort;
      const month = date.monthShort;
      
      if (!errorAnalysis.temporalWeaknesses[dayOfWeek]) {
        errorAnalysis.temporalWeaknesses[dayOfWeek] = { total: 0, errors: 0 };
      }
      errorAnalysis.temporalWeaknesses[dayOfWeek].total++;
      if (!positionCorrect) {
        errorAnalysis.temporalWeaknesses[dayOfWeek].errors++;
      }

      // PH√ÇN T√çCH THEO TH√ÅNG
      if (!errorAnalysis.temporalWeaknesses[month]) {
        errorAnalysis.temporalWeaknesses[month] = { total: 0, errors: 0 };
      }
      errorAnalysis.temporalWeaknesses[month].total++;
      if (!positionCorrect) {
        errorAnalysis.temporalWeaknesses[month].errors++;
      }
    }

    // T√çNH TO√ÅN K·∫æT QU·∫¢
    errorAnalysis.overallAccuracy = totalPredictions > 0 ? correctPredictions / totalPredictions : 0;
    errorAnalysis.totalAnalyzed = totalPredictions;

    // X√ÅC ƒê·ªäNH V·ªä TR√ç Y·∫æU NH·∫§T
    const positionStats = {};
    errorAnalysis.weakPositions.forEach(pos => {
      positionStats[pos] = (positionStats[pos] || 0) + 1;
    });

    errorAnalysis.weakPositions = Object.entries(positionStats)
      .sort((a, b) => b[1] - a[1])
      .map(([pos, count]) => ({
        position: pos,
        errorCount: count,
        errorRate: count / totalPredictions,
        weight: 1 + (count / totalPredictions) * 2 // T·ª∑ l·ªá sai c√†ng cao -> weight c√†ng l·ªõn
      }));

    // T√çNH T·ª∂ L·ªÜ L·ªñI THEO TH·ªúI GIAN
    for (const [key, data] of Object.entries(errorAnalysis.temporalWeaknesses)) {
      data.errorRate = data.errors / data.total;
      data.weight = 1 + data.errorRate; // T·ª∑ l·ªá l·ªói cao -> weight cao
    }

    console.log('üìä K·∫æT QU·∫¢ PH√ÇN T√çCH L·ªñI:');
    console.log(`- T·ªïng s·ªë d·ª± ƒëo√°n ƒë√£ ph√¢n t√≠ch: ${errorAnalysis.totalAnalyzed}`);
    console.log(`- ƒê·ªô ch√≠nh x√°c t·ªïng: ${(errorAnalysis.overallAccuracy * 100).toFixed(1)}%`);
    console.log(`- V·ªã tr√≠ y·∫øu nh·∫•t: ${errorAnalysis.weakPositions[0]?.position} (${(errorAnalysis.weakPositions[0]?.errorRate * 100).toFixed(1)}% sai)`);
    
    this.errorPatterns = errorAnalysis;
    return errorAnalysis;
  }

  getDefaultErrorPatterns() {
    return {
      weakPositions: [
        { position: 'pos1', errorRate: 0.7, weight: 2.4 },
        { position: 'pos2', errorRate: 0.6, weight: 2.2 },
        { position: 'pos3', errorRate: 0.5, weight: 2.0 },
        { position: 'pos4', errorRate: 0.4, weight: 1.8 },
        { position: 'pos5', errorRate: 0.3, weight: 1.6 }
      ],
      temporalWeaknesses: {},
      overallAccuracy: 0,
      totalAnalyzed: 0
    };
  }

  // =================================================================
  // T√çNH TR·ªåNG S·ªê TH√îNG MINH CHO T·ª™NG M·∫™U HU·∫§N LUY·ªÜN
  // =================================================================
  calculateSmartWeights(trainingData) {
    console.log('üéØ T√≠nh tr·ªçng s·ªë th√¥ng minh cho t·ª´ng m·∫´u hu·∫•n luy·ªán...');
    
    if (!this.errorPatterns) {
      console.log('‚ö†Ô∏è Ch∆∞a c√≥ ph√¢n t√≠ch l·ªói, s·ª≠ d·ª•ng weights m·∫∑c ƒë·ªãnh');
      return Array(trainingData.length).fill(1.0);
    }

    const weights = trainingData.map((sample, index) => {
      let weight = 1.0; // Weight m·∫∑c ƒë·ªãnh

      try {
        // 1. TƒÇNG TR·ªåNG S·ªê CHO C√ÅC M·∫™U LI√äN QUAN ƒê·∫æN V·ªä TR√ç Y·∫æU
        this.errorPatterns.weakPositions.forEach(weakPos => {
          if (weakPos.errorRate > 0.5) { // Ch·ªâ x√©t c√°c v·ªã tr√≠ sai > 50%
            weight += weakPos.weight * 0.3;
          }
        });

        // 2. TƒÇNG TR·ªåNG S·ªê CHO C√ÅC M·∫™U C√ì FEATURES ƒê·∫∂C BI·ªÜT
        const featureVector = sample.inputSequence.flat();
        const hasExtremeValues = featureVector.some(val => Math.abs(val) > 0.8);
        if (hasExtremeValues) {
          weight += 0.4; // C√°c features c·ª±c tr·ªã th∆∞·ªùng quan tr·ªçng
        }

        // 3. TƒÇNG TR·ªåNG S·ªê CHO C√ÅC M·∫™U C√ì PATTERN PH·ª®C T·∫†P
        const featureComplexity = this.calculateFeatureComplexity(featureVector);
        weight += featureComplexity * 0.2;

        // 4. GI·∫¢M TR·ªåNG S·ªê CHO C√ÅC M·∫™U QU√Å ƒê∆†N GI·∫¢N
        const simpleFeatureCount = featureVector.filter(val => Math.abs(val) < 0.1).length;
        if (simpleFeatureCount > featureVector.length * 0.8) {
          weight *= 0.8; // Gi·∫£m weight cho m·∫´u qu√° ƒë∆°n gi·∫£n
        }

      } catch (error) {
        console.warn(`‚ö†Ô∏è L·ªói t√≠nh weight cho sample ${index}:`, error.message);
        weight = 1.0; // Fallback v·ªÅ weight m·∫∑c ƒë·ªãnh
      }

      return Math.min(Math.max(weight, 0.5), 3.0); // Gi·ªõi h·∫°n weight t·ª´ 0.5 ƒë·∫øn 3.0
    });

    console.log(`‚úÖ ƒê√£ t√≠nh weights cho ${weights.length} m·∫´u:`);
    console.log(`- Weight trung b√¨nh: ${(weights.reduce((a, b) => a + b, 0) / weights.length).toFixed(2)}`);
    console.log(`- Weight min: ${Math.min(...weights).toFixed(2)}, max: ${Math.max(...weights).toFixed(2)}`);

    return weights;
  }

  calculateFeatureComplexity(featureVector) {
    // T√≠nh ƒë·ªô ph·ª©c t·∫°p c·ªßa feature vector d·ª±a tr√™n variance
    const mean = featureVector.reduce((a, b) => a + b, 0) / featureVector.length;
    const variance = featureVector.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / featureVector.length;
    return Math.min(variance * 10, 1.0); // Chu·∫©n h√≥a v·ªÅ 0-1
  }

  // =================================================================
  // HU·∫§N LUY·ªÜN V·ªöI SMART WEIGHTING
  // =================================================================
  async trainModelWithSmartWeights(trainingData) {
    console.log('üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán v·ªõi Smart Weighting...');
    
    // PH√ÇN T√çCH L·ªñI TR∆Ø·ªöC KHI HU·∫§N LUY·ªÜN
    await this.analyzeHistoricalErrors();
    
    // T√çNH TR·ªåNG S·ªê TH√îNG MINH
    const weights = this.calculateSmartWeights(trainingData);
    
    const inputs = trainingData.map(d => d.inputSequence);
    const targets = trainingData.map(d => d.targetArray);

    const inputTensor = tf.tensor3d(inputs, [inputs.length, SEQUENCE_LENGTH, this.inputNodes]);
    const targetTensor = tf.tensor2d(targets, [targets.length, OUTPUT_NODES]);
    const weightTensor = tf.tensor1d(weights);

    console.log('üîß B·∫Øt ƒë·∫ßu training v·ªõi smart weights...');
    
    const history = await this.model.fit(inputTensor, targetTensor, {
      epochs: EPOCHS,
      batchSize: Math.min(BATCH_SIZE, inputs.length),
      validationSplit: 0.1,
      verbose: 0, // ‚úÖ T·∫ÆT TI·∫æN TR√åNH ƒê·ªÇ KH√îNG L·ªñI TICK
      sampleWeight: weightTensor,
      callbacks: {
        onEpochEnd: (epoch, logs) => {
          if (isNaN(logs.loss)) {
            console.error('‚ùå NaN loss detected! Stopping training.');
            this.model.stopTraining = true;
          } else if (epoch % 10 === 0) {
            console.log(`üìà Epoch ${epoch + 1}: Loss = ${logs.loss.toFixed(4)}, Val Loss = ${logs.val_loss?.toFixed(4) || 'N/A'}`);
          }
        }
      }
    });

    // GI·∫¢I PH√ìNG B·ªò NH·ªö
    inputTensor.dispose();
    targetTensor.dispose();
    weightTensor.dispose();

    console.log('‚úÖ Hu·∫•n luy·ªán v·ªõi Smart Weighting ho√†n t·∫•t!');
    return history;
  }

  // =================================================================
  // C√ÅC PH∆Ø∆†NG TH·ª®C G·ªêC - GI·ªÆ NGUY√äN NH∆ØNG TH√äM VERBOSE: 0
  // =================================================================
  async runAdvancedTraining() {
    console.log('üöÄ B·∫Øt ƒë·∫ßu Advanced Training...');
    
    const trainingData = await this.prepareTrainingData();
    
    const result = await this.advancedTrainer.trainWithAdvancedStrategies(
      trainingData, 
      ['ensemble', 'augmentation']
    );
    
    if (result.type === 'ensemble') {
      this.ensembleModels = result.models;
      console.log(`‚úÖ ƒê√£ train ${result.models.length} models cho ensemble`);
    } else {
      this.model = result.model;
      await this.saveModel();
    }
    
    return {
      message: 'Advanced training ho√†n t·∫•t',
      strategy: 'ensemble + augmentation',
      modelsCount: result.models?.length || 1
    };
  }

  async advancedPredict(inputSequence) {
    if (this.ensembleModels && this.ensembleModels.length > 0) {
      return await this.advancedTrainer.ensemblePredict(inputSequence);
    } else {
      return await this.predict(inputSequence);
    }
  }

  async buildModel(inputNodes) {
    console.log(`üèóÔ∏è X√¢y d·ª±ng model v·ªõi ${inputNodes} features...`);
    this.inputNodes = inputNodes;

    const model = tf.sequential();

    model.add(tf.layers.lstm({
      units: 32,
      returnSequences: false,
      inputShape: [SEQUENCE_LENGTH, inputNodes],
      kernelInitializer: 'glorotNormal',
      recurrentInitializer: 'orthogonal',
      kernelRegularizer: tf.regularizers.l2({l2: 0.001}),
      recurrentRegularizer: tf.regularizers.l2({l2: 0.001}),
      kernelConstraint: tf.constraints.maxNorm({maxValue: 1}),
      recurrentConstraint: tf.constraints.maxNorm({maxValue: 1})
    }));

    model.add(tf.layers.batchNormalization());
    model.add(tf.layers.dropout({rate: 0.2}));
    
    model.add(tf.layers.dense({
      units: 16,
      activation: 'relu',
      kernelInitializer: 'glorotNormal',
      kernelRegularizer: tf.regularizers.l2({l2: 0.001})
    }));

    model.add(tf.layers.dense({
      units: OUTPUT_NODES,
      activation: 'sigmoid',
      kernelInitializer: 'glorotNormal'
    }));
    
    model.summary();

    const optimizer = tf.train.adam(0.0005);
    
    model.compile({
      optimizer: tf.train.adam(0.0005),
      loss: 'meanSquaredError',
      metrics: []
    });

    this.model = model;
    return this.model;
  }

  async trainModel(trainingData) {
    const { inputs, targets } = trainingData;
    
    console.log('üîç Ki·ªÉm tra cu·ªëi c√πng tr∆∞·ªõc khi training:');
    console.log('- Inputs length:', inputs.length);
    console.log('- Targets length:', targets.length);
    
    const inputTensor = tf.tensor3d(inputs, [inputs.length, SEQUENCE_LENGTH, this.inputNodes]);
    const targetTensor = tf.tensor2d(targets, [targets.length, OUTPUT_NODES]);

    const optimizer = tf.train.adam(0.0005);
    
    this.model.compile({
      optimizer: optimizer,
      loss: 'binaryCrossentropy',
      metrics: []
    });

    const history = await this.model.fit(inputTensor, targetTensor, {
      epochs: EPOCHS,
      batchSize: Math.min(BATCH_SIZE, inputs.length),
      validationSplit: 0.1,
      verbose: 0, // ‚úÖ T·∫ÆT TI·∫æN TR√åNH
      callbacks: {
        onEpochEnd: (epoch, logs) => {
          if (isNaN(logs.loss)) {
            console.error('‚ùå NaN loss detected! Stopping training.');
            this.model.stopTraining = true;
          } else {
            console.log(`Epoch ${epoch + 1}: Loss = ${logs.loss.toFixed(4)}`);
          }
        }
      }
    });

    inputTensor.dispose();
    targetTensor.dispose();

    return history;
  }

  async predict(inputSequence) {
    const inputTensor = tf.tensor3d([inputSequence], [1, SEQUENCE_LENGTH, this.inputNodes]);
    const prediction = this.model.predict(inputTensor);
    const output = await prediction.data();
    prediction.dispose();
    inputTensor.dispose();
    return Array.from(output);
  }

  prepareTarget(gdbString) {
    const target = Array(OUTPUT_NODES).fill(0);
    gdbString.split('').forEach((digit, index) => {
      const d = parseInt(digit);
      if (!isNaN(d) && index < 5) {
        target[index * 10 + d] = 0.99;
      }
    });
    return target;
  }

  // =================================================================
  // PH∆Ø∆†NG TH·ª®C CH√çNH - S·ª¨A ƒê·ªîI ƒê·ªÇ D√ôNG SMART WEIGHTING
  // =================================================================
  async runHistoricalTraining() {
    console.log('üîî [TensorFlow Service] B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán L·ªãch s·ª≠ v·ªõi Smart Weighting...');
   
    const trainingData = await this.prepareTrainingData();
    if (trainingData.length === 0 || trainingData.some(d => d.inputSequence.length !== SEQUENCE_LENGTH || d.inputSequence.flat().some(isNaN))) {
      throw new Error('D·ªØ li·ªáu training r·ªóng ho·∫∑c ch·ª©a gi√° tr·ªã kh√¥ng h·ª£p l·ªá.');
    }
    
    await this.buildModel(this.inputNodes);
    
    this.model.compile({
      optimizer: tf.train.adam({learningRate: 0.0005}),
      loss: 'binaryCrossentropy',
      metrics: []
    });
    
    console.log('‚úÖ Model ƒë√£ ƒë∆∞·ª£c compile. B·∫Øt ƒë·∫ßu qu√° tr√¨nh training v·ªõi Smart Weighting...');
    
    // ‚úÖ S·ª¨ D·ª§NG SMART WEIGHTING THAY V√å TRAINING TH√îNG TH∆Ø·ªúNG
    await this.trainModelWithSmartWeights(trainingData);
   
    await this.saveModel();
    
    return {
      message: `Hu·∫•n luy·ªán v·ªõi Smart Weighting ho√†n t·∫•t. ƒê√£ x·ª≠ l√Ω ${trainingData.length} chu·ªói, ${EPOCHS} epochs.`,
      sequences: trainingData.length,
      epochs: EPOCHS,
      featureSize: this.inputNodes,
      modelName: NN_MODEL_NAME,
      smartWeighting: true
    };
  }

  async runLearning() {
    console.log('üîî [TensorFlow Service] Learning from new results...');
    
    if (!this.model) {
      const modelLoaded = await this.loadModel();
      if (!modelLoaded) {
        throw new Error('Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y ch·∫°y hu·∫•n luy·ªán l·ªãch s·ª≠ tr∆∞·ªõc.');
      }
    }

    const predictionsToLearn = await NNPrediction.find({ danhDauDaSo: false }).lean();
    if (predictionsToLearn.length === 0) {
      return { message: 'Kh√¥ng c√≥ d·ª± ƒëo√°n m·ªõi n√†o ƒë·ªÉ h·ªçc.' };
    }

    const results = await Result.find().sort({ 'ngay': 1 }).lean();
    const grouped = {};
    results.forEach(r => {
      if (!grouped[r.ngay]) grouped[r.ngay] = [];
      grouped[r.ngay].push(r);
    });

    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    
    let learnedCount = 0;
    const trainingData = [];

    for (const pred of predictionsToLearn) {
      const targetDayStr = pred.ngayDuDoan;
      const targetDayIndex = days.indexOf(targetDayStr);

      if (targetDayIndex >= SEQUENCE_LENGTH) {
        const actualResult = (grouped[targetDayStr] || []).find(r => r.giai === 'ƒêB');
        
        if (actualResult?.so && String(actualResult.so).length >= 5) {
          const sequenceDays = days.slice(targetDayIndex - SEQUENCE_LENGTH, targetDayIndex);
          const previousDays = [];
          const inputSequence = sequenceDays.map(day => {
            const dayResults = grouped[day] || [];
            const prevDays = previousDays.slice();
            previousDays.push(dayResults);
            
            const basicFeatures = this.featureService.extractAllFeatures(dayResults, prevDays, day);
            const advancedFeatures = this.advancedFeatureEngineer.extractPremiumFeatures(dayResults, prevDays);
            
            let finalFeatureVector = [...basicFeatures, ...Object.values(advancedFeatures).flat()];
            
            const EXPECTED_SIZE = 346;
            if (finalFeatureVector.length !== EXPECTED_SIZE) {
              if (finalFeatureVector.length > EXPECTED_SIZE) {
                finalFeatureVector = finalFeatureVector.slice(0, EXPECTED_SIZE);
              } else {
                finalFeatureVector = [...finalFeatureVector, ...Array(EXPECTED_SIZE - finalFeatureVector.length).fill(0)];
              }
            }
            
            return finalFeatureVector;
          });

          const totalValues = inputSequence.flat().length;
          const expectedValues = SEQUENCE_LENGTH * 346;
          
          if (totalValues !== expectedValues) {
            console.error(`‚ùå [Learning] L·ªói dimension: c√≥ ${totalValues} values, c·∫ßn ${expectedValues} values`);
            continue;
          }

          const targetGDBString = String(actualResult.so).padStart(5, '0');
          const targetArray = this.prepareTarget(targetGDBString);
          
          trainingData.push({ inputSequence, targetArray });
          learnedCount++;
        }
      }
      await NNPrediction.updateOne({ _id: pred._id }, { danhDauDaSo: true });
    }

    if (trainingData.length > 0) {
      console.log(`üéØ [Learning] B·∫Øt ƒë·∫ßu h·ªçc t·ª´ ${trainingData.length} chu·ªói d·ªØ li·ªáu m·ªõi`);
      
      const inputs = trainingData.map(d => d.inputSequence);
      const targets = trainingData.map(d => d.targetArray);

      const inputTensor = tf.tensor3d(inputs, [inputs.length, SEQUENCE_LENGTH, this.inputNodes]);
      const targetTensor = tf.tensor2d(targets, [targets.length, OUTPUT_NODES]);

      await this.model.fit(inputTensor, targetTensor, {
        epochs: 3,
        batchSize: Math.min(BATCH_SIZE, inputs.length),
        validationSplit: 0.1,
        verbose: 0 // ‚úÖ T·∫ÆT TI·∫æN TR√åNH
      });

      inputTensor.dispose();
      targetTensor.dispose();

      await this.saveModel();
      console.log(`‚úÖ [Learning] ƒê√£ h·ªçc xong t·ª´ ${learnedCount} k·∫øt qu·∫£ m·ªõi`);
    } else {
      console.log('‚ÑπÔ∏è [Learning] Kh√¥ng c√≥ d·ªØ li·ªáu training h·ª£p l·ªá ƒë·ªÉ h·ªçc');
    }
    
    return { message: `TensorFlow LSTM ƒë√£ h·ªçc xong. ƒê√£ x·ª≠ l√Ω ${learnedCount} k·∫øt qu·∫£ m·ªõi.` };
  }

  // =================================================================
  // C√ÅC PH∆Ø∆†NG TH·ª®C C√íN L·∫†I - GI·ªÆ NGUY√äN
  // =================================================================
  async prepareTrainingData() {
    console.log('üìù B·∫Øt ƒë·∫ßu chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán...');
    const results = await Result.find().sort({ 'ngay': 1 }).lean();
    
    console.log(`üìä T·ªïng s·ªë b·∫£n ghi trong DB: ${results.length}`);

    if (results.length < SEQUENCE_LENGTH + 1) {
      throw new Error(`Kh√¥ng ƒë·ªß d·ªØ li·ªáu. C·∫ßn √≠t nh·∫•t ${SEQUENCE_LENGTH + 1} ng√†y.`);
    }

    const grouped = {};
    results.forEach(r => {
      if (!grouped[r.ngay]) grouped[r.ngay] = [];
      grouped[r.ngay].push(r);
    });

    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    const trainingData = [];

    for (let i = 0; i < days.length - SEQUENCE_LENGTH; i++) {
      const sequenceDaysStrings = days.slice(i, i + SEQUENCE_LENGTH);
      const targetDayString = days[i + SEQUENCE_LENGTH];
      
      const inputSequence = [];
      let sequenceValid = true;

      for(let j = 0; j < SEQUENCE_LENGTH; j++) {
        const currentDayForFeature = grouped[sequenceDaysStrings[j]] || [];
        const dateStr = sequenceDaysStrings[j];
        
        const previousDaysForBasicFeatures = days.slice(0, i + j).map(day => grouped[day] || []);
        const previousDaysForAdvancedFeatures = previousDaysForBasicFeatures.slice().reverse();

        const basicFeatures = this.featureService.extractAllFeatures(currentDayForFeature, previousDaysForBasicFeatures, dateStr);
        const advancedFeatures = this.advancedFeatureEngineer.extractPremiumFeatures(currentDayForFeature, previousDaysForAdvancedFeatures);
        
        let finalFeatureVector = [...basicFeatures, ...Object.values(advancedFeatures).flat()];
        
        const hasInvalid = finalFeatureVector.some(val => 
          isNaN(val) || val === null || val === undefined || !isFinite(val)
        );
        
        if (hasInvalid) {
          sequenceValid = false;
          break;
        }
        
        const EXPECTED_SIZE = 346;
        if (finalFeatureVector.length !== EXPECTED_SIZE) {
          if (finalFeatureVector.length > EXPECTED_SIZE) {
            finalFeatureVector = finalFeatureVector.slice(0, EXPECTED_SIZE);
          } else {
            finalFeatureVector = [...finalFeatureVector, ...Array(EXPECTED_SIZE - finalFeatureVector.length).fill(0)];
          }
        }
        
        inputSequence.push(finalFeatureVector);
      }

      if (!sequenceValid) continue;

      const targetGDB = (grouped[targetDayString] || []).find(r => r.giai === 'ƒêB');
      if (targetGDB?.so && String(targetGDB.so).length >= 5) {
        const targetGDBString = String(targetGDB.so).padStart(5, '0');
        const targetArray = this.prepareTarget(targetGDBString);

        const invalidTargets = targetArray.filter(val => isNaN(val) || val === null || val === undefined);
        if (invalidTargets.length > 0) continue;

        trainingData.push({ inputSequence, targetArray });
      }
    }

    if (trainingData.length > 0) {
      this.inputNodes = trainingData[0].inputSequence[0].length;
      console.log(`‚úÖ ƒê√£ chu·∫©n b·ªã ${trainingData.length} chu·ªói d·ªØ li·ªáu h·ª£p l·ªá`);
    } else {
      throw new Error("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu training h·ª£p l·ªá sau khi ki·ªÉm tra.");
    }

    return trainingData;
  }

  dateKey(s) {
    if (!s || typeof s !== 'string') return '';
    const parts = s.split('/');
    return parts.length !== 3 ? s : `${parts[2]}-${parts[1].padStart(2, '0')}-${parts[0].padStart(2, '0')}`;
  }

  async saveModel() {
    if (!this.model) throw new Error('Kh√¥ng c√≥ model ƒë·ªÉ l∆∞u.');

    console.log(`üíæ [SaveModel] Chu·∫©n b·ªã l∆∞u model l√™n GCS...`);
    
    const modelGcsPath = `models/${NN_MODEL_NAME}`;
    
    const ioHandler = getGcsIoHandler(modelGcsPath);

    const saveResult = await this.model.save(ioHandler);

    const modelInfo = {
        modelName: NN_MODEL_NAME,
        inputNodes: this.inputNodes,
        savedAt: new Date().toISOString(),
        gcsPath: `gs://${bucketName}/${modelGcsPath}`
    };

    await NNState.findOneAndUpdate(
        { modelName: NN_MODEL_NAME },
        { 
            state: modelInfo,
            modelArtifacts: saveResult
        },
        { upsert: true, new: true }
    );
    
    console.log(`‚úÖ [SaveModel] Model ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng l√™n GCS t·∫°i: ${modelInfo.gcsPath}`);
  }

  async loadModel() {
    console.log(`üîç [LoadModel] Chu·∫©n b·ªã t·∫£i model t·ª´ GCS...`);

    const modelState = await NNState.findOne({ modelName: NN_MODEL_NAME }).lean();
    
    if (modelState && modelState.state && modelState.state.gcsPath) {
        const modelGcsPath = modelState.state.gcsPath.replace(`gs://${bucketName}/`, '');

        try {
            const ioHandler = getGcsIoHandler(modelGcsPath);
            
            this.model = await tf.loadLayersModel(ioHandler);
            this.inputNodes = modelState.state.inputNodes;
            
            console.log(`‚úÖ [LoadModel] Model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ GCS: ${modelState.state.gcsPath}`);
            this.model.summary();
            return true;
        } catch (error) {
            console.error(`‚ùå [LoadModel] L·ªói khi t·∫£i model t·ª´ GCS:`, error);
            return false;
        }
    } else {
        console.log('‚ùå [LoadModel] Kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng d·∫´n GCS trong database. Model c·∫ßn ƒë∆∞·ª£c hu·∫•n luy·ªán l·∫°i.');
        return false;
    }
  }

  async runNextDayPrediction() {
    console.log('üîî [TensorFlow Service] Generating next day prediction...');
    
    if (!this.model) {
      const modelLoaded = await this.loadModel();
      if (!modelLoaded) {
        throw new Error('Model ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. H√£y ch·∫°y hu·∫•n luy·ªán tr∆∞·ªõc.');
      }
    }

    const results = await Result.find().lean();
    if (results.length < SEQUENCE_LENGTH) {
      throw new Error(`Kh√¥ng ƒë·ªß d·ªØ li·ªáu. C·∫ßn √≠t nh·∫•t ${SEQUENCE_LENGTH} ng√†y.`);
    }

    const grouped = {};
    results.forEach(r => {
      if (!grouped[r.ngay]) grouped[r.ngay] = [];
      grouped[r.ngay].push(r);
    });

    const days = Object.keys(grouped).sort((a, b) => this.dateKey(a).localeCompare(this.dateKey(b)));
    const latestSequenceDays = days.slice(-SEQUENCE_LENGTH);

    console.log(`üîç Chu·∫©n b·ªã d·ªØ li·ªáu d·ª± ƒëo√°n t·ª´ ${latestSequenceDays.length} ng√†y g·∫ßn nh·∫•t`);

    const previousDays = [];
    const inputSequence = latestSequenceDays.map(day => {
      const dayResults = grouped[day] || [];
      const prevDays = previousDays.slice();
      previousDays.push(dayResults);
      
      const basicFeatures = this.featureService.extractAllFeatures(dayResults, prevDays, day);
      const advancedFeatures = this.advancedFeatureEngineer.extractPremiumFeatures(dayResults, prevDays);
      
      let finalFeatureVector = [...basicFeatures, ...Object.values(advancedFeatures).flat()];
      
      const EXPECTED_SIZE = 346;
      if (finalFeatureVector.length !== EXPECTED_SIZE) {
        if (finalFeatureVector.length > EXPECTED_SIZE) {
          finalFeatureVector = finalFeatureVector.slice(0, EXPECTED_SIZE);
        } else {
          finalFeatureVector = [...finalFeatureVector, ...Array(EXPECTED_SIZE - finalFeatureVector.length).fill(0)];
        }
      }
      
      return finalFeatureVector;
    });

    const totalValues = inputSequence.flat().length;
    const expectedValues = SEQUENCE_LENGTH * 346;
    
    if (totalValues !== expectedValues) {
      throw new Error(`L·ªói dimension: c√≥ ${totalValues} values, c·∫ßn ${expectedValues} values`);
    }

    const output = await this.predict(inputSequence);
    const prediction = this.decodeOutput(output);

    const latestDay = latestSequenceDays[latestSequenceDays.length - 1];
    const nextDayStr = DateTime.fromFormat(latestDay, 'dd/MM/yyyy').plus({ days: 1 }).toFormat('dd/MM/yyyy');

    await NNPrediction.findOneAndUpdate(
      { ngayDuDoan: nextDayStr },
      { ngayDuDoan: nextDayStr, ...prediction, danhDauDaSo: false },
      { upsert: true, new: true }
    );

    return {
      message: `TensorFlow LSTM ƒë√£ t·∫°o d·ª± ƒëo√°n cho ng√†y ${nextDayStr}.`,
      ngayDuDoan: nextDayStr
    };
  }

  decodeOutput(output) {
    const prediction = { pos1: [], pos2: [], pos3: [], pos4: [], pos5: [] };
    for (let i = 0; i < 5; i++) {
      const positionOutput = output.slice(i * 10, (i + 1) * 10);
      const digitsWithValues = positionOutput
        .map((value, index) => ({ digit: String(index), value }))
        .sort((a, b) => b.value - a.value)
        .slice(0, 5)
        .map(item => item.digit);
      prediction[`pos${i + 1}`] = digitsWithValues;
    }
    return prediction;
  }
}

module.exports = TensorFlowService;
